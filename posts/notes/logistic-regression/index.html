<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">
    <meta name="color-scheme" content="light dark">

    

    <meta name="author" content="Muhammad Assagaf">
    <meta name="description" content="statistical model that models the probability of an event taking place by having the log-odds of an event be a linear combination of one or more independent variables. since logistic regression fit a model in the form $p(y|x)$ directly, it&rsquo;s called discriminative approach.
$$ p(y|x,w) = Ber(y|sigm(w^Tx))$$ where sigm is a sigmoid function: $$sigm(x) = \frac{1}{1 &#43; e^{-x}}$$
model fitting    MLE    the negative log-likelihood for logistic regression is given by $$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) &#43; (1-y_i)log(1-\mu_i)$$ this is also called **cross entropy** error function.">
    <meta name="keywords" content="blog,developer,personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Logistic Regression"/>
<meta name="twitter:description" content="statistical model that models the probability of an event taking place by having the log-odds of an event be a linear combination of one or more independent variables. since logistic regression fit a model in the form $p(y|x)$ directly, it&rsquo;s called discriminative approach.
$$ p(y|x,w) = Ber(y|sigm(w^Tx))$$ where sigm is a sigmoid function: $$sigm(x) = \frac{1}{1 &#43; e^{-x}}$$
model fitting    MLE    the negative log-likelihood for logistic regression is given by $$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) &#43; (1-y_i)log(1-\mu_i)$$ this is also called **cross entropy** error function."/>

    <meta property="og:title" content="Logistic Regression" />
<meta property="og:description" content="statistical model that models the probability of an event taking place by having the log-odds of an event be a linear combination of one or more independent variables. since logistic regression fit a model in the form $p(y|x)$ directly, it&rsquo;s called discriminative approach.
$$ p(y|x,w) = Ber(y|sigm(w^Tx))$$ where sigm is a sigmoid function: $$sigm(x) = \frac{1}{1 &#43; e^{-x}}$$
model fitting    MLE    the negative log-likelihood for logistic regression is given by $$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) &#43; (1-y_i)log(1-\mu_i)$$ this is also called **cross entropy** error function." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/notes/logistic-regression/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-10T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-12-10T00:00:00&#43;00:00" />



    <title>
  Logistic Regression · Muhammad
</title>

    
      <link rel="canonical" href="../../../posts/notes/logistic-regression/">
    

    <link rel="preload" href="../../../fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>

    
      
      
      <link rel="stylesheet" href="../../../css/coder.min.0e5ce5b959a68dfe0232c6ddcec1e8ef154517c968464707f3181c437fe611c0.css" integrity="sha256-DlzluVmmjf4CMsbdzsHo7xVFF8loRkcH8xgcQ3/mEcA=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="../../../css/coder-dark.min.717236c74e0a5208ef73964a9f44c6b443b689a95b270d8b2a40d0c012460dac.css" integrity="sha256-cXI2x04KUgjvc5ZKn0TGtEO2ialbJw2LKkDQwBJGDaw=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="../../../img/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="../../../img/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="../../../images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/apple-touch-icon.png">

    <meta name="generator" content="Hugo 0.83.1" />
  </head>

  
  
    
  
  <body class="preload-transitions colorscheme-auto">
    
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="../../../">
      Muhammad
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="../../../about/">About Me</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="../../../posts/notes/">Personal notes</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="../../../posts/notes/logistic-regression/">
              Logistic Regression
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2022-12-10T00:00:00Z'>
                December 10, 2022
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              One-minute read
            </span>
          </div>
          
          
          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="../../../tags/classification/">classification</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="../../../tags/statistical-model/">statistical model</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="../../../tags/machine-learning/">machine-learning</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <hr>
<p>statistical model  that models the probability of an event taking place by having the log-odds of an event be a linear combination of one or more independent variables.
since logistic regression fit a model in the form  $p(y|x)$ directly, it&rsquo;s called <strong>discriminative</strong> approach.</p>
<p>$$ p(y|x,w) = Ber(y|sigm(w^Tx))$$
where sigm is a sigmoid function:
$$sigm(x) = \frac{1}{1 + e^{-x}}$$</p>
<h2 id="model-fitting">
  model fitting
  <a class="heading-link" href="#model-fitting">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<h3 id="mle">
  MLE
  <a class="heading-link" href="#mle">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>the negative <strong>log-likelihood</strong> for logistic regression is given by
$$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) + (1-y_i)log(1-\mu_i)$$
this is also called **cross entropy** error function.</p>
<p>we can introduce a <strong>bias</strong> term if we want to penalize the <strong>w</strong> and make it smaller, smaller <strong>w</strong> will give as simpler hypothesis and less prone to overfitting (this is called <a href="../../../posts/notes/regularization/">regularization</a>)</p>
<p>example of NLL with L2 Regularization
$$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) + (1-y_i)log(1-\mu_i) + \lambda (w^Tw)$$</p>
<h1 id="references">
  References
  <a class="heading-link" href="#references">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p><a href="https://en.wikipedia.org/wiki/Logistic_regression">https://en.wikipedia.org/wiki/Logistic_regression</a></p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
    integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
        <p>More to Come</p>
      
      
        ©
        
        2023
         Muhammad Assagaf 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
    </section>
  </footer>


    </main>

    
      
      <script src="../../../js/coder.min.03b17769f4f91ae35667e1f2a1ca8c16f50562576cf90ff32b3179926914daa5.js" integrity="sha256-A7F3afT5GuNWZ&#43;HyocqMFvUFYlds&#43;Q/zKzF5kmkU2qU="></script>
    

    

    

    

    

    

    

    

    
  </body>

</html>
