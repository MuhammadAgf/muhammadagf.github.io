<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">
    <meta name="color-scheme" content="light dark">

    

    <meta name="author" content="Muhammad Assagaf">
    <meta name="description" content="source: Feature Stores for ML, 2021
Batch Inference and ETL Pipelines    Batch inference involves using machine learning models to generate predictions for a large number of data points in a batch or scheduled manner, without the need for real-time information.
Batch Inference    Batch inference is used when predictions are not required immediately and can be generated on a recurring schedule, such as daily or weekly.">
    <meta name="keywords" content="blog,developer,personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ml Batch Processing"/>
<meta name="twitter:description" content="source: Feature Stores for ML, 2021
Batch Inference and ETL Pipelines    Batch inference involves using machine learning models to generate predictions for a large number of data points in a batch or scheduled manner, without the need for real-time information.
Batch Inference    Batch inference is used when predictions are not required immediately and can be generated on a recurring schedule, such as daily or weekly."/>

    <meta property="og:title" content="Ml Batch Processing" />
<meta property="og:description" content="source: Feature Stores for ML, 2021
Batch Inference and ETL Pipelines    Batch inference involves using machine learning models to generate predictions for a large number of data points in a batch or scheduled manner, without the need for real-time information.
Batch Inference    Batch inference is used when predictions are not required immediately and can be generated on a recurring schedule, such as daily or weekly." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://muhammadagf.github.io/posts/notes/ml-batch-inference/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-19T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-06-19T00:00:00&#43;00:00" />



    <title>
  Ml Batch Processing · Muhammad
</title>

    
      <link rel="canonical" href="https://muhammadagf.github.io/posts/notes/ml-batch-inference/">
    

    <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>

    
      
      
      <link rel="stylesheet" href="/css/coder.min.728f45c9eaff821acb9cccdb60c81cf16be81bd890ee22cc5b5f4dbf276a082f.css" integrity="sha256-co9Fyer/ghrLnMzbYMgc8WvoG9iQ7iLMW19NvydqCC8=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.aa883b9ce35a8ff4a2a5008619005175e842bb18a8a9f9cc2bbcf44dab2d91fa.css" integrity="sha256-qog7nONaj/SipQCGGQBRdehCuxioqfnMK7z0Tastkfo=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    <meta name="generator" content="Hugo 0.83.1" />
  </head>

  
  
    
  
  <body class="preload-transitions colorscheme-auto">
    
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Muhammad
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About Me</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/portofolio/">Portofolio</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/notes/">Personal notes</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://muhammadagf.github.io/posts/notes/ml-batch-inference/">
              Ml Batch Processing
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2023-06-19T00:00:00Z'>
                June 19, 2023
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              2-minute read
            </span>
          </div>
          
          
          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/mlops/">mlops</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/ml-model-deployment/">ML Model Deployment</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <hr>
<p><img src="/images/obsidian/Pasted%20image%2020230619131458.png" alt="img">
source: Feature Stores for ML, 2021</p>
<h1 id="batch-inference-and-etl-pipelines">
  Batch Inference and ETL Pipelines
  <a class="heading-link" href="#batch-inference-and-etl-pipelines">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>Batch inference involves using machine learning models to generate predictions for a large number of data points in a batch or scheduled manner, without the need for real-time information.</p>
<h2 id="batch-inference">
  Batch Inference
  <a class="heading-link" href="#batch-inference">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p><strong>Batch inference is used when predictions are not required immediately and can be generated on a recurring schedule, such as daily or weekly.</strong> It is useful for scenarios like batch recommendations, where historical data is used to generate predictions without real-time input.
Note: it can also be combined with Online Prediction (<a href="https://muhammadagf.github.io/posts/notes/ml-model-serving/">ML model serving</a>) as part of the <strong>feature store</strong>.</p>
<style type="text/css">.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style>
<div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice tip" >
<p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"></use></svg></span>Tip</p><p>Advantages of batch inference include the ability to use complex models without inference time constraints and the absence of the need for prediction caching.</p></div>

<h3 id="use-cases-of-batch-inference">
  Use Cases of Batch Inference
  <a class="heading-link" href="#use-cases-of-batch-inference">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<ol>
<li><strong>E-commerce Site</strong>: Generate new product recommendations on a recurring schedule, storing predictions for easy retrieval instead of generating them in real time.</li>
<li><strong>Sentiment Analysis</strong>: Predict sentiment (positive, neutral, negative) based on user reviews, using batch predictions on a recurring schedule to improve services or products over time.</li>
<li><strong>Demand Forecasting</strong>: Estimate product demand on a daily basis for inventory and ordering optimization, leveraging time series models in batch prediction systems.</li>
</ol>
<h2 id="etl-pipelines">
  ETL Pipelines
  <a class="heading-link" href="#etl-pipelines">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<ul>
<li>Before using data for batch predictions, it needs to be extracted from various sources, transformed, and loaded into an output destination.</li>
<li>ETL pipelines consist of processes for extracting, transforming, and loading data, typically in a distributed manner for parallel processing.</li>
<li>Data extraction and transformation can be performed using frameworks like Apache Spark or Google Cloud Dataflow, utilizing the Apache Beam programming paradigm.</li>
<li>Transformed data is stored in databases or data warehouses before being sent for batch prediction.</li>
</ul>
<h3 id="frameworks-for-batch-processing-and-etl">
  Frameworks for Batch Processing and ETL
  <a class="heading-link" href="#frameworks-for-batch-processing-and-etl">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<ul>
<li>In general Data can come from 2 types of sources:
<ul>
<li><strong>Batch data</strong> available in huge volumes in CSV files, XML files, JSON, APIs, or data lakes like Google Cloud Storage.</li>
<li><strong>Streaming data</strong> from continuously updating sources (or sensors) can be handled by Apache Kafka, Google Cloud Pub/Sub or similar Message Queue tools.</li>
</ul>
</li>
<li>Apache Spark and Google Cloud Dataflow (based on Apache Beam) are commonly used engines for ETL processing..</li>
<li>Transformed data can be stored in data warehouses like BigQuery or data lakes before being used for batch prediction or other analytics purposes.</li>
</ul>
<hr>
<h1 id="references">
  References
  <a class="heading-link" href="#references">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<ol>
<li><a href="https://community.deeplearning.ai/t/mlep-course-4-lecture-notes/54456">https://community.deeplearning.ai/t/mlep-course-4-lecture-notes/54456</a></li>
</ol>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
    integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
        <p>More to Come</p>
      
      
        ©
        
        2024
         Muhammad Assagaf 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
    </section>
  </footer>


    </main>

    
      
      <script src="/js/coder.min.03b17769f4f91ae35667e1f2a1ca8c16f50562576cf90ff32b3179926914daa5.js" integrity="sha256-A7F3afT5GuNWZ&#43;HyocqMFvUFYlds&#43;Q/zKzF5kmkU2qU="></script>
    

    

    

    

    

    

    

    

    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-K5E9PZY0GT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-K5E9PZY0GT');
</script>

  </body>

</html>
