<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mlops on Muhammad</title>
    <link>https://muhammadagf.github.io/tags/mlops/</link>
    <description>Recent content in mlops on Muhammad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 13 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muhammadagf.github.io/tags/mlops/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pipeline Prallelism</title>
      <link>https://muhammadagf.github.io/posts/notes/pipeline-prallelism/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/pipeline-prallelism/</guid>
      <description>ETL Problem:    Transformations and preprocessing tasks add overhead to the training input pipeline.
Commons ETL: source: https://community.deeplearning.ai/t/mlep-course-3-lecture-notes/54454
Problem:
 Input pipelines are needed to supply enough data fast enough to keep accelerators busy. Pre-processing tasks and data size can add overhead to the training input pipeline.  Improved Input Pipeline:     Parallel processing of data is essential to utilize compute, IO, and network resources effectively.</description>
    </item>
    
    <item>
      <title>Distributed Training</title>
      <link>https://muhammadagf.github.io/posts/notes/distributed-training/</link>
      <pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/distributed-training/</guid>
      <description>Distributed training allows for training huge models and speeding up the training process.
Types:    Data parallelism    Dividing data into partitions and copying the complete model to all workers. Each worker operates on a different partition, and model updates are synchronized across workers. source: https://community.deeplearning.ai/t/mlep-course-3-lecture-notes/
  Synchronous training: (example: all-reduce architecture) Workers train on it&amp;rsquo;s current mini-batches of data, apply updates, and wait for updates from other workers before proceeding.</description>
    </item>
    
    <item>
      <title>Ml Deployment Patterns And Degrees Of Automation</title>
      <link>https://muhammadagf.github.io/posts/notes/ml-deployment-patterns-and-degrees-of-automation/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ml-deployment-patterns-and-degrees-of-automation/</guid>
      <description>Use Cases:    Offering a new product or capability:    When introducing a new service like speech recognition, a common design pattern is to start with a small amount of traffic and gradually increase it.
Automating or assisting existing tasks:    If a task previously performed by humans, like inspecting smartphones for defects, can be automated or assisted by a learning algorithm, a shadow mode deployment can be used.</description>
    </item>
    
    <item>
      <title>Machine Learning Lifecycle</title>
      <link>https://muhammadagf.github.io/posts/notes/machine-learning-lifecycle/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/machine-learning-lifecycle/</guid>
      <description>Scoping:
 Define the project. Decide on key metrics. Estimate resources, and timeline.    Data:
 Define data.  establish a baselines, and address data labeling inconsistencies. obtaining data.   Standardize data labeling conventions.  Advanced Labeling Technique:  Semi-supervised Learning Active Learning Weak Supervision        Modeling: To get started in modelling there are 3 key input: Code (algorithm/model), Hyperparameter, Data.</description>
    </item>
    
    <item>
      <title>Ml Model Deployment</title>
      <link>https://muhammadagf.github.io/posts/notes/ml-model-deployment/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ml-model-deployment/</guid>
      <description>ML/Statistical Issue:    Given a data X, model F and output y, we have a data distribution changes Issue called: Concept drift and Data drift.
Concept Drift    refer to when desired mapping from X to y changes. for example due to inflation, same size of the house can be priced higher.
Data Drift    refer to when the input distribution (X) changes. for example when most people suddenly build a smaller house.</description>
    </item>
    
  </channel>
</rss>
