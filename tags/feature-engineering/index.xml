<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>feature engineering on Muhammad</title>
    <link>https://muhammadagf.github.io/tags/feature-engineering/</link>
    <description>Recent content in feature engineering on Muhammad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 09 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muhammadagf.github.io/tags/feature-engineering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Missing Values</title>
      <link>https://muhammadagf.github.io/posts/notes/missing-values/</link>
      <pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/missing-values/</guid>
      <description>Type of Missing Data      Structural deficiencies in the data For example: when null means &amp;ldquo;no&amp;rdquo; or 0.
  Random occurrences
  Missing Completely at Random (MCAR) The likelihood of a missing results is equal for all data points. the missing value are independent of the data.
  Missing at Random (MAR) The likelihood of a missing results is not equal for all data points.</description>
    </item>
    
    <item>
      <title>Greedy Feature Selection</title>
      <link>https://muhammadagf.github.io/posts/notes/greedy-feature-selection/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/greedy-feature-selection/</guid>
      <description>Simple Filters    The most basic approach to feature selection is to screen the predictors to see if any have a relationship with the outcome prior to including them in a model.
Some popular techniques:
Categorical Feature     when the target is categorical the relationship between feature and outcome forms a contingency tables.  3 or more level for the feature: can use chi-squared test or exact methods 2 level for the feature: can use odds-ratio   when the target is numeric  2 level for the feature: basic t-test can be calculated 3 level for the feature: traditional ANOVA F-statistic can be calculated    Numeric Feature     when the target is categorical similar as when feature is categorical and the target is numeric (but the role is reversed) correlation-adjusted t-scores are a good alternative to simple ANOVA statistics.</description>
    </item>
    
    <item>
      <title>Feature Selection</title>
      <link>https://muhammadagf.github.io/posts/notes/feature-selection/</link>
      <pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/feature-selection/</guid>
      <description>Goals     Some models (such as SVM and Neural Networks) are sensitive to irrelevant features/predictors. superfluous features can sink predictive performance in some situations. Some models (such as logistic regression) are vulnerable to correlated features. removing features can reduce cost and it make scientific sense to include the minimum possible set that provides acceptable results.  Classes of Feature Selection Method    in general feature selection method can be divided by 3</description>
    </item>
    
  </channel>
</rss>
