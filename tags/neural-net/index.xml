<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neural-net on Muhammad</title>
    <link>https://muhammadagf.github.io/tags/neural-net/</link>
    <description>Recent content in neural-net on Muhammad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 21 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muhammadagf.github.io/tags/neural-net/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bakcpropagation</title>
      <link>https://muhammadagf.github.io/posts/notes/bakcpropagation/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/bakcpropagation/</guid>
      <description>Wikipedia: Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example. Recursive application of chain-rule along the computational graph to compute the gradients for all input and parameters.
Computational Graph    A computational graph is defined as a directed graph where the nodes correspond to mathematical operations. Computational graphs are a way of expressing and evaluating a mathematical expression.</description>
    </item>
    
    <item>
      <title>Knowledge Distillation</title>
      <link>https://muhammadagf.github.io/posts/notes/knowledge-distillation/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/knowledge-distillation/</guid>
      <description>Idea: Duplicate the performance of a complex model (teacher) to a simpler model (student)
Teacher and Student     The teacher model is trained first using a standard objective function to maximize its accuracy or a similar metric. The student model, on the other hand, aims to learn transferable knowledge from the teacher by matching the probability distribution of the teacher&amp;rsquo;s predictions.  Dark Knowledge     Improve softness of the teacher&amp;rsquo;s distribution with Softmax Temperature (T) As T grows, you get more insight about which classes the teacher finds similar to the predicted one $$p_i = \frac{exp(\frac{Z_i}{T})}{\sum_j exp(\frac{Z_j}{T})}$$  Techniques     Approach 1: Weigh objectives (Student and teacher) and combine during backprob Approach 2: Compare distributions of the predictions (student and teacher) using KL divergence   References     https://www.</description>
    </item>
    
    <item>
      <title>Pipeline Prallelism</title>
      <link>https://muhammadagf.github.io/posts/notes/pipeline-prallelism/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/pipeline-prallelism/</guid>
      <description>ETL Problem:    Transformations and preprocessing tasks add overhead to the training input pipeline.
Commons ETL: source: https://community.deeplearning.ai/t/mlep-course-3-lecture-notes/54454
Problem:
 Input pipelines are needed to supply enough data fast enough to keep accelerators busy. Pre-processing tasks and data size can add overhead to the training input pipeline.  Improved Input Pipeline:     Parallel processing of data is essential to utilize compute, IO, and network resources effectively.</description>
    </item>
    
    <item>
      <title>Distributed Training</title>
      <link>https://muhammadagf.github.io/posts/notes/distributed-training/</link>
      <pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/distributed-training/</guid>
      <description>Distributed training allows for training huge models and speeding up the training process.
Types:    Data parallelism    Dividing data into partitions and copying the complete model to all workers. Each worker operates on a different partition, and model updates are synchronized across workers. source: https://community.deeplearning.ai/t/mlep-course-3-lecture-notes/
  Synchronous training: (example: all-reduce architecture) Workers train on it&amp;rsquo;s current mini-batches of data, apply updates, and wait for updates from other workers before proceeding.</description>
    </item>
    
    <item>
      <title>Model Resource Optimization</title>
      <link>https://muhammadagf.github.io/posts/notes/model-resource-optimization/</link>
      <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/model-resource-optimization/</guid>
      <description>Background    Machine learning is increasingly integrated into mobile, IoT, and embedded applications, with billions of devices already in use.
Reasons for Deploying Models on Device
 Advances in Machine Learning Research: Research enables running inference locally on low-power devices, making it feasible to incorporate machine learning as part of a device&amp;rsquo;s core functionality. Decreasing Hardware Costs: Lower hardware costs allow for affordable devices and higher volume production, making on-device machine learning more accessible.</description>
    </item>
    
    <item>
      <title>Adam (Adaptive Moment Estimation)</title>
      <link>https://muhammadagf.github.io/posts/notes/adam/</link>
      <pubDate>Wed, 10 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/adam/</guid>
      <description>$$w_t = w_{t-1} - \eta \ \dfrac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$ with: $$\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1}$$ $$\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}$$ this is for bias correction for the fact that first and second moment estimates start at zero.
given: $$m_t = \beta_1m_{t-1} + (1 - \beta_1)\ \delta w_t$$ $$v_t = \beta_2v_{t-1} + (1 - \beta_2)\ \delta w_t^2$$
 $m_t$ is momentum. $v_t$ takes the idea from AdaGrad / RMSProp  parameter:</description>
    </item>
    
    <item>
      <title>Recurrent Neural Net</title>
      <link>https://muhammadagf.github.io/posts/notes/recurrent-neural-net/</link>
      <pubDate>Sun, 07 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/recurrent-neural-net/</guid>
      <description>Vanilla RNN:    $$h_t = F(h_{t-1}, x_t)$$ $$h_t = activation(W_h\ h_{t-1} + W_x\ x_t)$$ $$y_t = W_y \ h_t$$
image source: https://youtu.be/6niqTuYFZLQ
Problem:     hard to remember long sentences Vanishing and Exploding Gradient problem  LSTM:    image from https://colah.github.io/posts/2015-08-Understanding-LSTMs/
Forget gate:    since i&amp;rsquo;s using sigmoid function the output would be 0-1 (decide how much we should forget the long term memory, if 0 forget)</description>
    </item>
    
    <item>
      <title>Convolutional Neural Net</title>
      <link>https://muhammadagf.github.io/posts/notes/convolutional-neural-net/</link>
      <pubDate>Sat, 06 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/convolutional-neural-net/</guid>
      <description>CNNs use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers. Why CNN?     curse of dimensionality  for a single 32x32 pixel image, it will have 3 channel of 1024 pixels. Fully Connected Neural Networks (FCNNs) treat every pixel as an independent input and do not consider the spatial relationship between them. FCNNs require a large number of parameters, making them computationally expensive and prone to overfitting.</description>
    </item>
    
  </channel>
</rss>
