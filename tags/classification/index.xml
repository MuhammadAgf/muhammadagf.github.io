<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>classification on Muhammad</title>
    <link>https://muhammadagf.github.io/tags/classification/</link>
    <description>Recent content in classification on Muhammad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 25 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muhammadagf.github.io/tags/classification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Decision Tree</title>
      <link>https://muhammadagf.github.io/posts/notes/decision-tree/</link>
      <pubDate>Sat, 25 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/decision-tree/</guid>
      <description>Greedy, Top Down, Recursive Partitioning
CART Algorithm    CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.
 get list of split candidates for each split candidates, calculate the purity of the split select the best split, and check if stopping criterion is met repeat step 1  Classification    if a target is a classification with k number of classes, for node m, let prediction: $$p_{mk} = \frac{1}{n_m} \sum_{y\in Node_m} I(y=k)$$ usually, we use Gini Impurity to meassure the purity of each node or split candidates: $$IG(Node) = 1-\sum_{i=0}^k p(i)^2$$ or for binary classification = (1 - probability of yes squared + probability of no squared)</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://muhammadagf.github.io/posts/notes/logistic-regression/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/logistic-regression/</guid>
      <description>statistical model that models the probability of an event taking place by having the log-odds of an event be a linear combination of one or more independent variables. since logistic regression fit a model in the form $p(y|x)$ directly, it&amp;rsquo;s called discriminative approach.
$$ p(y|x,w) = Ber(y|sigm(w^Tx))$$ where sigm is a sigmoid function: $$sigm(x) = \frac{1}{1 + e^{-x}}$$
model fitting    MLE    the negative log-likelihood for logistic regression is given by $$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) + (1-y_i)log(1-\mu_i)$$ this is also called **cross entropy** error function.</description>
    </item>
    
  </channel>
</rss>
