<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Muhammad</title>
    <link>https://muhammadagf.github.io/</link>
    <description>Recent content on Muhammad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 31 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muhammadagf.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://muhammadagf.github.io/portofolio/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/portofolio/</guid>
      <description>Projects:  MLOPS with Kubernetes 0 to 1 Address Elements Extraction | NLP, Competition RealTime Vehicle Counting and Tracking | CV   Talks:  How Data Science is Boosting Ruangguru to Solve Complex EdTech Problems Recommender System in Education Modeling Student’s Understanding Level of a Concept with Knowledge Tracing      Projects:    MLOPS with Kubernetes 0 to 1    Repository aimed to help anyone who get stuck and doesn&amp;rsquo;t know where to start with MLOPS with kubernetes.</description>
    </item>
    
    <item>
      <title>Central Limit Theorem</title>
      <link>https://muhammadagf.github.io/posts/notes/central-limit-theorem/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/central-limit-theorem/</guid>
      <description>In probability theory, the central limit theorem (CLT) establishes that, in many situations, for identically distributed independent samples, the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed.
sampling distribution of the mean    given a population with mean $\mu$ and standard deviation $\sigma$, the sampling distribution of the mean approaches: $$\mathcal{N}(\mu, \frac{\sigma}{\sqrt{n}}) $$ as n, the sample size, increases.</description>
    </item>
    
    <item>
      <title>Gradient Boosting</title>
      <link>https://muhammadagf.github.io/posts/notes/gradient-boosting/</link>
      <pubDate>Mon, 27 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/gradient-boosting/</guid>
      <description>Algorithm     Initialize model with a constant value $F_0$ $$F_0(x) = arg\ min\ \gamma \sum_{i}^n L(y_i, \gamma)$$ *initiate the base model (for regression usually $F_0$ is $mean(y)$ For $m$ to $M$:  compute pseudo-residuals: $$r_{im} = \Biggl[ \frac{\partial L(y_i, F(x_i)) }{\partial F(x_i)} \Biggr]_{F(x) = F_{m-1}(x)}$$ for $i = 1, . . ., n.$ Fit a weak learner to pseudo-residuals, i.e. train it using $r_{im}$ as the target.</description>
    </item>
    
    <item>
      <title>Decision Tree</title>
      <link>https://muhammadagf.github.io/posts/notes/decision-tree/</link>
      <pubDate>Sat, 25 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/decision-tree/</guid>
      <description>Greedy, Top Down, Recursive Partitioning
CART Algorithm    CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.
 get list of split candidates for each split candidates, calculate the purity of the split select the best split, and check if stopping criterion is met repeat step 1  Classification    if a target is a classification with k number of classes, for node m, let prediction: $$p_{mk} = \frac{1}{n_m} \sum_{y\in Node_m} I(y=k)$$ usually, we use Gini Impurity to meassure the purity of each node or split candidates: $$IG(Node) = 1-\sum_{i=0}^k p(i)^2$$ or for binary classification = (1 - probability of yes squared + probability of no squared)</description>
    </item>
    
    <item>
      <title>Muhammad Abdullah Assagaf</title>
      <link>https://muhammadagf.github.io/about/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/about/</guid>
      <description>Hi, I&amp;rsquo;m Muhammad
Currently, I hold the position of Senior Data Scientist at Noice, where my responsibilities include developing a data platform project and implementing MLOps processes. I have successfully increased the click-through rate by creating and designing scoring algorithms for our search engine.
In my prior role at Ruangguru as a Senior Data Scientist, I managed and co-developed several machine learning deployment projects. Additionally, I collaborated with the sales team to create a user scoring model that increased subscription rates.</description>
    </item>
    
    <item>
      <title>Standard Error</title>
      <link>https://muhammadagf.github.io/posts/notes/standard-error/</link>
      <pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/standard-error/</guid>
      <description>usually standard error is: standard deviations of the sampling mean
in essence it gives us how much variations in our &amp;ldquo;means&amp;rdquo; if we took a bunch of independent measurement samples.
formula: $$SE(\hat{x} ) = \frac{\sigma}{\sqrt{n}} $$ $\hat{x}$ is sample mean $\sigma$ is population std $n$ is number of sample
derivation:    $$SE(\hat{x}) = Std(samplemean) = \sqrt{VAR[\hat{X}]}$$ $$\sqrt{VAR[\frac{T}{n}]} = \sqrt{\frac{1}{n^2}VAR[T]} = \sqrt{\frac{1}{n}\sigma^2} = \frac{\sigma}{\sqrt{n}}$$ $T$ is sum of the mean from random variable $\hat{X}$ notice that $T$ consist of sum of iid ( independent and identically distributed) RV because we get T by sampling from the same distribution over and over again (bootstraping).</description>
    </item>
    
    <item>
      <title>Binomial Variable</title>
      <link>https://muhammadagf.github.io/posts/notes/binomial-variable/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/binomial-variable/</guid>
      <description>special class of random variable
X = number of success given N trial with p probability of each success on each trial where each trial are independent with each other.
properties:     made of independent trials each trials can be classified as success or failure fixed # of trials probability of success on each trial is constant  Binomial distribution    special case of probability mass function from a binomial variable pmf if the probability of success is p, the probability of having exactly k success given n trial is given by:</description>
    </item>
    
    <item>
      <title>Permutations And Combinations</title>
      <link>https://muhammadagf.github.io/posts/notes/permutations-and-combinations/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/permutations-and-combinations/</guid>
      <description>permutation    how many way we can arange thing if we care about the order? $$_kP_n = \frac{n!}{(n-k)!}$$ example: how many way 5 person can be seated to seat numbered from 1-3 = $_3P_5$ answer = 5 way for the first seat, 4 way for the 2nd seat, 3 way for the 3rd seat $$\frac{5!}{(5-3)!} = \frac{5!}{2!} = 5 * 4 * 3 = 60$$
combination    how many way we can get item/thing if we care about the order?</description>
    </item>
    
    <item>
      <title>Probability Mass Function (Pmf)</title>
      <link>https://muhammadagf.github.io/posts/notes/probability-mass-function/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/probability-mass-function/</guid>
      <description>PMF for a disscrete random variable X: $p(X)$
Mean (Expectation)    *the sum of the weighted possible values of X (sum of $p(X=x)x$ ) .notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Tip
$$\mu = E[X] = \sum_{x} x p(X=x)$$
 Variance    measure the spread of a PMF (Average Squared Distance from the mean) Tip</description>
    </item>
    
    <item>
      <title>Random Variable</title>
      <link>https://muhammadagf.github.io/posts/notes/random-variable/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/random-variable/</guid>
      <description>mapping outcomes -&amp;gt; numbers from a random process
Discrete    Suppose X is a discrete event, we denote probability of $X=x$ by $p(X=x)$, or just $p(x)$ for short. Here $p(x)$ is called a probability mass function or pmf.
Continuous    Suppose X is some uncertain continuous quantity. The probability that X lies in any interval $a \leq X \leq b$ can be computed as follows:</description>
    </item>
    
    <item>
      <title>Z-Score</title>
      <link>https://muhammadagf.github.io/posts/notes/z-score/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/z-score/</guid>
      <description>how many $\sigma$ (standard deviation) away from the mean from a normal distribution ? $$ z = \frac{x-\mu}{\sigma}$$
usage    comparing 2 distribution for example Juwan took LSAT and MCAT with score: 172 on the LSAT and 37 on the MCAT. summary stats of LSAT and MCAT:
   Exam mean $\mu$ std $\sigma$     LSAT 151 10   MCAT 25.1 6.4    which exam did he do relatively better on?</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://muhammadagf.github.io/posts/notes/linear-regression/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/linear-regression/</guid>
      <description>to answer:
 Is there a relationship between feature advertising budget and sales? How strong is the relationship between advertising budget and sales? Which media are associated with sales? How large is the association between each medium and slaes? How accurately we can predict future sales? Is there synergy among the advertising media?  Single Linear Regression    $$ Y \approx \beta_0 + \beta_1 X$$
RSS (Residual Sum Squared)    let $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$ be the prediction of $y$ based on the $i$th value of $X$.</description>
    </item>
    
    <item>
      <title>Greedy Feature Selection</title>
      <link>https://muhammadagf.github.io/posts/notes/greedy-feature-selection/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/greedy-feature-selection/</guid>
      <description>Simple Filters    The most basic approach to feature selection is to screen the predictors to see if any have a relationship with the outcome prior to including them in a model.
Some popular techniques:
Categorical Feature     when the target is categorical the relationship between feature and outcome forms a contingency tables.  3 or more level for the feature: can use chi-squared test or exact methods 2 level for the feature: can use odds-ratio   when the target is numeric  2 level for the feature: basic t-test can be calculated 3 level for the feature: traditional ANOVA F-statistic can be calculated    Numeric Feature     when the target is categorical similar as when feature is categorical and the target is numeric (but the role is reversed) correlation-adjusted t-scores are a good alternative to simple ANOVA statistics.</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>https://muhammadagf.github.io/posts/notes/entropy/</link>
      <pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/entropy/</guid>
      <description>the entropy of a random variable $X$ with distribution $p$, denoted by $H(X)$ or sometimes $H(p)$, is a measure of it&amp;rsquo;s uncertainty. In particular, for a discrete variable with K states, it is defined by .notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Tip
$$ H(X) \triangleq - \sum_{i=1}^K p(X=k) log(p(X=k)) $$
 in essence entropy is calculating the expected surprise we get from a RV intuitively the value of a surprise is inverse probability of an event, or mathematicaly: $$ surprise(p(X=k)) = log(\frac{1}{p(X=k)})$$</description>
    </item>
    
    <item>
      <title>Feature Selection</title>
      <link>https://muhammadagf.github.io/posts/notes/feature-selection/</link>
      <pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/feature-selection/</guid>
      <description>Goals     Some models (such as SVM and Neural Networks) are sensitive to irrelevant features/predictors. superfluous features can sink predictive performance in some situations. Some models (such as logistic regression) are vulnerable to correlated features. removing features can reduce cost and it make scientific sense to include the minimum possible set that provides acceptable results.  Classes of Feature Selection Method    in general feature selection method can be divided by 3</description>
    </item>
    
    <item>
      <title>Bias Variance Tradeoff</title>
      <link>https://muhammadagf.github.io/posts/notes/bias-variance-tradeoff/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/bias-variance-tradeoff/</guid>
      <description>.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Note
MSE = bias$^2$ + variance
 image source: https://www.codingninjas.com/codestudio/library/bias-variance-tradeoff
Bias (Underfitting)    $$Bias[Y_{pred}|Y_{true}] = E[Y_{pred}] - Y_{true}$$ the **bias** error is an error from incorrect assumption in the learning algorithm. high bias can cause an algorithm to miss the relevant relations between features and target output (**underfitting**) (systematic off-the mark)</description>
    </item>
    
    <item>
      <title>K-Means Clustering</title>
      <link>https://muhammadagf.github.io/posts/notes/k-means-clustering/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/k-means-clustering/</guid>
      <description>How it works:
 K centroid are created randomly (based predefined number of K) by selecting K random data as centroid K-means algorithm will allocate each data point in the data to the nearest centroid based on some distance metrics (usually euclidean distance) K-means take the mean of each data in each cluster and use that mean as the new centroid (not necessarily assigned to a single datapoint) K-means then repeat step 2 and 3 until stoping criterion is met (eg.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://muhammadagf.github.io/posts/notes/logistic-regression/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/logistic-regression/</guid>
      <description>statistical model that models the probability of an event taking place by having the log-odds of an event be a linear combination of one or more independent variables. since logistic regression fit a model in the form $p(y|x)$ directly, it&amp;rsquo;s called discriminative approach.
$$ p(y|x,w) = Ber(y|sigm(w^Tx))$$ where sigm is a sigmoid function: $$sigm(x) = \frac{1}{1 + e^{-x}}$$
model fitting    MLE    the negative log-likelihood for logistic regression is given by $$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) + (1-y_i)log(1-\mu_i)$$ this is also called **cross entropy** error function.</description>
    </item>
    
    <item>
      <title>Regularization</title>
      <link>https://muhammadagf.github.io/posts/notes/regularization/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/regularization/</guid>
      <description>add penalty to the loss function and reduce the value of weight. (introduce bias) $$ Regularized loss = loss + penalty $$ example:
L1 Regularization    or called Lasso add “absolute value of magnitude” coefficient as a penalty term to the loss function (minimize sum of of coefficients/weights)
$$ penalty = \lambda ||w|| = \lambda \sum_{j=1}^M |w|$$
pros:     robust to outliers works best when your model contains a lot of useless features (built-in feature selection) preferred when having a high number of features as it&amp;rsquo;s provide sparse solution  cons:     if there is a high group of higly correlated variables, L1 tend to select 1 variable from the group and ignore the others  L2 Regularization    or called Ridge add “squared magnitude” coefficient as a penalty term to the loss function (minimize sum of square of coefficients/weights)</description>
    </item>
    
  </channel>
</rss>
