<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Muhammad</title>
    <link>https://muhammadagf.github.io/</link>
    <description>Recent content on Muhammad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 23 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muhammadagf.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ab-Testing</title>
      <link>https://muhammadagf.github.io/posts/notes/ab-testing/</link>
      <pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ab-testing/</guid>
      <description>Designing an A/B Testing    Selecting Metrics for the Experiment    .notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Tip
Experiment Metrics Criteria:
 Measurable within the experiment period. Attributable to the change of the experiment variants. Sensitive enough to detect change that matter in timely fashion  Focus on Driver metrics and Guardrail metrics not the Goal metrics</description>
    </item>
    
    <item>
      <title>Product-Analyst</title>
      <link>https://muhammadagf.github.io/posts/notes/product-analyst/</link>
      <pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/product-analyst/</guid>
      <description>As a Product Analyst, your primary goal is to understand how people use our product, so we can improve their experience and drive positive business outcomes.
Definition    A way to tell story about how people are using your product so that you can understand how to improve their experience and positively impact your business metrics.
on a pirate funnel (AAARRR aka Awareness, Acquisition, Activation, Retention, Revenue, Referral) Product analytics will help you develop a detailed understanding of how users behave when they first access your product.</description>
    </item>
    
    <item>
      <title>Measuring Product Improvement</title>
      <link>https://muhammadagf.github.io/posts/notes/measuring-product-improvement/</link>
      <pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/measuring-product-improvement/</guid>
      <description>Before launching a feature, it is crucial to analyze existing data to make informed decisions about what to build. This process often leads to questions about the potential impact of adding, changing, or improving a feature.
Understanding Feature Change    To evaluate a feature change, it is essential to understand why someone proposed the feature and what goals it aims to achieve (similar to measuring success in a product.</description>
    </item>
    
    <item>
      <title>Investigating Metrics In A Product</title>
      <link>https://muhammadagf.github.io/posts/notes/investigating-metrics-in-a-product/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/investigating-metrics-in-a-product/</guid>
      <description>To investigate Why is the metric for a feature X dropping by Y%
Introduction    It&amp;rsquo;s crucial to investigate and understand why certain metrics in a product are dropping. This guide will provide you with a step-by-step framework to approach such investigations and help you uncover the reasons behind the metric decline.
Step 1: Clarify    To begin the investigation, you need to clarify certain aspects related to the dropping metric:</description>
    </item>
    
    <item>
      <title>Bakcpropagation</title>
      <link>https://muhammadagf.github.io/posts/notes/bakcpropagation/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/bakcpropagation/</guid>
      <description>Wikipedia: Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example. Recursive application of chain-rule along the computational graph to compute the gradients for all input and parameters.
Computational Graph    A computational graph is defined as a directed graph where the nodes correspond to mathematical operations. Computational graphs are a way of expressing and evaluating a mathematical expression.</description>
    </item>
    
    <item>
      <title>Ml Batch Processing</title>
      <link>https://muhammadagf.github.io/posts/notes/ml-batch-inference/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ml-batch-inference/</guid>
      <description>source: Feature Stores for ML, 2021
Batch Inference and ETL Pipelines    Batch inference involves using machine learning models to generate predictions for a large number of data points in a batch or scheduled manner, without the need for real-time information.
Batch Inference    Batch inference is used when predictions are not required immediately and can be generated on a recurring schedule, such as daily or weekly.</description>
    </item>
    
    <item>
      <title>Ml Experiment Tracking</title>
      <link>https://muhammadagf.github.io/posts/notes/ml-experiment-tracking/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ml-experiment-tracking/</guid>
      <description>Experiment tracking is the process of managing all the different experiments and their components, such as parameters, metrics, models and other artifacts.
Running experiments without proper tools can easily result in a disorganized and unmanageable workflow, even for simple projects. As a data scientist, it is essential to choose the right experiment tracking tool that best fits your needs and workflow.
Why Tracking an experiment:     Small changes in models and data can significantly impact performance and resource requirements, emphasizing the importance of tracking even minor changes.</description>
    </item>
    
    <item>
      <title>Mlops</title>
      <link>https://muhammadagf.github.io/posts/notes/mlops/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/mlops/</guid>
      <description>Introduction to MLOps    MLOps, or Machine Learning Operations, refers to the practices and processes involved in managing and deploying machine learning models in production. It combines the principles of software engineering and DevOps with the unique challenges presented by machine learning systems.
source: https://neptune.ai/blog/mlops
Challenges in ML Engineering    There are several challenges in ML engineering that drive the need for MLOps:
  Slow Deployment: Deploying ML models to production can take days, weeks or even months.</description>
    </item>
    
    <item>
      <title>Ml Model Serving</title>
      <link>https://muhammadagf.github.io/posts/notes/ml-model-serving/</link>
      <pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ml-model-serving/</guid>
      <description>Infrastructure for Model Serving    two main options for infrastructure:
  On-Premises: In this approach, you have all the hardware and software required for running your models on your own premises.
 pros: provides you with complete control and flexibility to adapt to changes quickly. cons: it can be complicated and expensive to procure, install, configure, and maintain the hardware infrastructure.    Cloud Provider: Alternatively, you can outsource your infrastructure needs to a cloud provider like Amazon Web Services, Google Cloud Platform, or Microsoft Azure.</description>
    </item>
    
    <item>
      <title>Model Debugging</title>
      <link>https://muhammadagf.github.io/posts/notes/model-debugging/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/model-debugging/</guid>
      <description>Model performance analysis goes beyond simple metrics and focuses on model robustness. Model debugging is a discipline that aims to improve model robustness. .notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Tip
Robustness refers to the consistency of a model&amp;rsquo;s accuracy when features change.
 Objectives of model debugging include: improving model transparency, reducing discrimination, addressing vulnerabilities, and managing performance decay.</description>
    </item>
    
    <item>
      <title>Model Remediation</title>
      <link>https://muhammadagf.github.io/posts/notes/model-remediation/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/model-remediation/</guid>
      <description>ways to improve model robustness:
Data Augmentation    data augmentation can help generalize the model and reduce sensitivity and useful for correcting unbalanced data.
 Ensure training data accurately represents the requests your model will receive. Generate data through techniques like generative or interpretive methods, or by adding noise..  Explainable AI (XAI)     Understand the inner workings of your model to improve robustness. Tools and techniques are available to enhance model interpretability.</description>
    </item>
    
    <item>
      <title>Sensitivity Analysis</title>
      <link>https://muhammadagf.github.io/posts/notes/sensitivity-analysis/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/sensitivity-analysis/</guid>
      <description>Sensitivity analysis helps understand a model&amp;rsquo;s behavior by examining the impact of each feature on predictions.
It involves changing a single feature while keeping others constant and observing the resulting model outputs. The magnitude of change in predictions indicates the feature&amp;rsquo;s influence.
Techniques for sensitivity analysis include:
 Random attacks: Generating random input data to test model outputs and uncover unexpected bugs. Partial dependence plots: Show marginal effect of 1 or 2 feature and it&amp;rsquo;s effect to the result of the model  PDPbox and PyCEbox are open-source packages for creating partial dependence plots.</description>
    </item>
    
    <item>
      <title>Knowledge Distillation</title>
      <link>https://muhammadagf.github.io/posts/notes/knowledge-distillation/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/knowledge-distillation/</guid>
      <description>Idea: Duplicate the performance of a complex model (teacher) to a simpler model (student)
Teacher and Student     The teacher model is trained first using a standard objective function to maximize its accuracy or a similar metric. The student model, on the other hand, aims to learn transferable knowledge from the teacher by matching the probability distribution of the teacher&amp;rsquo;s predictions.  Dark Knowledge     Improve softness of the teacher&amp;rsquo;s distribution with Softmax Temperature (T) As T grows, you get more insight about which classes the teacher finds similar to the predicted one $$p_i = \frac{exp(\frac{Z_i}{T})}{\sum_j exp(\frac{Z_j}{T})}$$  Techniques     Approach 1: Weigh objectives (Student and teacher) and combine during backprob Approach 2: Compare distributions of the predictions (student and teacher) using KL divergence   References     https://www.</description>
    </item>
    
    <item>
      <title>Pipeline Prallelism</title>
      <link>https://muhammadagf.github.io/posts/notes/pipeline-prallelism/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/pipeline-prallelism/</guid>
      <description>ETL Problem:    Transformations and preprocessing tasks add overhead to the training input pipeline.
Commons ETL: source: https://community.deeplearning.ai/t/mlep-course-3-lecture-notes/54454
Problem:
 Input pipelines are needed to supply enough data fast enough to keep accelerators busy. Pre-processing tasks and data size can add overhead to the training input pipeline.  Improved Input Pipeline:     Parallel processing of data is essential to utilize compute, IO, and network resources effectively.</description>
    </item>
    
    <item>
      <title>Distributed Training</title>
      <link>https://muhammadagf.github.io/posts/notes/distributed-training/</link>
      <pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/distributed-training/</guid>
      <description>Distributed training allows for training huge models and speeding up the training process.
Types:    Data parallelism    Dividing data into partitions and copying the complete model to all workers. Each worker operates on a different partition, and model updates are synchronized across workers. source: https://community.deeplearning.ai/t/mlep-course-3-lecture-notes/
  Synchronous training: (example: all-reduce architecture) Workers train on it&amp;rsquo;s current mini-batches of data, apply updates, and wait for updates from other workers before proceeding.</description>
    </item>
    
    <item>
      <title>Active Learning</title>
      <link>https://muhammadagf.github.io/posts/notes/active-learning/</link>
      <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/active-learning/</guid>
      <description>Active learning is a method of intelligently sampling data to select the most informative unlabeled points for labeling. It helps in scenarios with limited data budget, imbalanced datasets, and when standard sampling techniques do not improve accuracy.
Active learning involves iteratively selecting the most uncertain data points for labeling to improve the model&amp;rsquo;s performance. Intelligent sampling techniques in active learning:     Margin sampling: Labels the most uncertain points based on their distance from the decision boundary.</description>
    </item>
    
    <item>
      <title>Dimensionality Reduction</title>
      <link>https://muhammadagf.github.io/posts/notes/dimensionality-reduction/</link>
      <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/dimensionality-reduction/</guid>
      <description>Not to be confused with feature selection. .notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Tip
Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension space
 Why is it a problem?     More dimensions → more features  Redundant / irrelevant features More noise added than signal Hard to interpret and visualize Hard to store and process data   Risk of overfitting our models Distances grow more and more alike No clear distinction between clustered objects Concentration phenomenon for Euclidean distance  Using the same number of data, there will be a point where increasing features will reduces model performances (irrelevant features can have effect to a model depend on the type of the model itself, see: feature selection) Source: https://community.</description>
    </item>
    
    <item>
      <title>Model Resource Optimization</title>
      <link>https://muhammadagf.github.io/posts/notes/model-resource-optimization/</link>
      <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/model-resource-optimization/</guid>
      <description>Background    Machine learning is increasingly integrated into mobile, IoT, and embedded applications, with billions of devices already in use.
Reasons for Deploying Models on Device
 Advances in Machine Learning Research: Research enables running inference locally on low-power devices, making it feasible to incorporate machine learning as part of a device&amp;rsquo;s core functionality. Decreasing Hardware Costs: Lower hardware costs allow for affordable devices and higher volume production, making on-device machine learning more accessible.</description>
    </item>
    
    <item>
      <title>Semi-Supervised Learning</title>
      <link>https://muhammadagf.github.io/posts/notes/semi-supervised-learning/</link>
      <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/semi-supervised-learning/</guid>
      <description>Semi-supervised learning is a technique that combines a small labeled dataset with a large unlabeled dataset to improve model performance. The process involves inferring labels for the unlabeled data based on how labeled classes are structured within the feature space.
Semi-supervised learning assumes that different label classes exhibit clustering or recognizable structure. source: https://www.altexsoft.com/blog/semi-supervised-learning/
Key Principle    Instead of adding tags to the entire dataset, you go through and hand-label just a small part of the data and use it to train a model, which then is applied to the ocean of unlabeled data.</description>
    </item>
    
    <item>
      <title>Data Distribution Changes</title>
      <link>https://muhammadagf.github.io/posts/notes/data-distribution-changes/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/data-distribution-changes/</guid>
      <description>Data Drift and Skew     Drift refers to changes in data over time. It occurs when there are changes in the statistical properties of the features between different time periods or data collections. Skew is the difference between two versions of the same dataset from different sources. It can be caused by changes in the data schema or distribution.  Model Decay and Data Issues     Model decay refers to the decline in model performance over time, which is often caused by data drift and concept drift.</description>
    </item>
    
    <item>
      <title>Feature Engineering</title>
      <link>https://muhammadagf.github.io/posts/notes/feature-engineering/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/feature-engineering/</guid>
      <description>Preprocessing      Data Cleansing: To eliminate or correct erroneous data by identifying and handling inconsistencies or outliers.
  Scaling and Normalizing: Numerical features often need to be scaled or normalized to ensure the models can learn effectively. Scaling adjusts the range of features, while normalization brings them within a specific range, such as between 0 and 1.
  dimensionality Reduction: Reducing the number of features by creating a lower-dimensional representation of the data.</description>
    </item>
    
    <item>
      <title>Data Augmentation</title>
      <link>https://muhammadagf.github.io/posts/notes/data-augmentation/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/data-augmentation/</guid>
      <description>Data augmentation is a valuable technique for increasing the amount of data available for training machine learning models, particularly for unstructured data like images, audio, and text.
However, when applying data augmentation, it is important to make thoughtful decisions. Here are some best practices to consider:
  Goal of Data Augmentation: The purpose of data augmentation is to generate examples that challenge the learning algorithm, while still being recognizable by humans or a baseline algorithm.</description>
    </item>
    
    <item>
      <title>Data Labeling Conventions</title>
      <link>https://muhammadagf.github.io/posts/notes/data-labeling-conventions/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/data-labeling-conventions/</guid>
      <description>Improving Label Consistency    To enhance the consistency of labels in a project, follow these steps:
  Multiple Labelers: If label consistency is a concern, select a few examples and have multiple labelers assign labels to them. This approach helps identify inconsistencies among labelers.
  Relabeling: Allow the same labeler to relabel an example after a break or time gap, encouraging consistency even within individual labelers.</description>
    </item>
    
    <item>
      <title>Obtaining Data</title>
      <link>https://muhammadagf.github.io/posts/notes/obtaining-data/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/obtaining-data/</guid>
      <description>How long should you spend obtaining data?    source: https://www.coursera.org/learn/introduction-to-machine-learning-in-production/lecture/Y7zTm/obtaining-data
  Define the data requirements: Determine what data is needed for the project, including the definition of the target variable (y) and input variables (x).
  Time investment: Consider the time spent on obtaining data in relation to the iterative process of machine learning. It is recommended to enter the iteration loop quickly by not spending excessive time on data collection initially.</description>
    </item>
    
    <item>
      <title>Error Analysis In Machine Learning</title>
      <link>https://muhammadagf.github.io/posts/notes/error-analysis-in-machine-learning/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/error-analysis-in-machine-learning/</guid>
      <description>When training a learning algorithm for the first time, it is expected that it won&amp;rsquo;t work perfectly right away. The key to improving its performance lies in error analysis. By analyzing the errors made by the algorithm, we can determine the most efficient use of our time in improving its performance.
Example: Speech Recognition Error Analysis    To illustrate the error analysis process, let&amp;rsquo;s consider an example of speech recognition.</description>
    </item>
    
    <item>
      <title>Establish A Baselines</title>
      <link>https://muhammadagf.github.io/posts/notes/establish-a-baselines/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/establish-a-baselines/</guid>
      <description>When starting a machine learning project, it is essential to establish a baseline level of performance before making improvements. This baseline serves as a point of comparison and helps determine where to focus efforts.
Establishing a Baseline     Determine major categories in your data. for speech recognition: (e.g., clear speech, speech with car noise, speech with people noise, low bandwidth audio). Measure accuracy for each category (e.</description>
    </item>
    
    <item>
      <title>Get Started In Modelling</title>
      <link>https://muhammadagf.github.io/posts/notes/get-started-in-modelling/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/get-started-in-modelling/</guid>
      <description>Tips for Getting Started    When starting a machine learning project, there are several tips to consider for efficient progress.
Getting Started on Modeling     Conduct a literature search to understand existing approaches and possibilities. Focus on practical implementations instead of the latest algorithms. Open source implementations can provide a baseline and facilitate quick start. A reasonable algorithm with good data often outperforms a cutting-edge algorithm with poor data.</description>
    </item>
    
    <item>
      <title>Ml Deployment Patterns And Degrees Of Automation</title>
      <link>https://muhammadagf.github.io/posts/notes/ml-deployment-patterns-and-degrees-of-automation/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ml-deployment-patterns-and-degrees-of-automation/</guid>
      <description>Use Cases:    Offering a new product or capability:    When introducing a new service like speech recognition, a common design pattern is to start with a small amount of traffic and gradually increase it.
Automating or assisting existing tasks:    If a task previously performed by humans, like inspecting smartphones for defects, can be automated or assisted by a learning algorithm, a shadow mode deployment can be used.</description>
    </item>
    
    <item>
      <title>Machine Learning Lifecycle</title>
      <link>https://muhammadagf.github.io/posts/notes/machine-learning-lifecycle/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/machine-learning-lifecycle/</guid>
      <description>Check: MLOps (mlops) as well.
  Scoping:
 Define the project. Decide on key metrics. Estimate resources, and timeline.    Data:
 Define data.  establish a baselines, and address data labeling inconsistencies. obtaining data.   Standardize data labeling conventions.  Advanced Labeling Technique:  Semi-supervised Learning Active Learning Weak Supervision        Modeling: To get started in modelling there are 3 key input: Code (algorithm/model), Hyperparameter, Data.</description>
    </item>
    
    <item>
      <title>Ml Model Deployment</title>
      <link>https://muhammadagf.github.io/posts/notes/ml-model-deployment/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/ml-model-deployment/</guid>
      <description>Issues:    ML/Statistical Issue:    Given a data X, model F and output y, we have a data distribution changes Issue called: Concept drift and Data drift.
Concept Drift    refer to when desired mapping from X to y changes. for example due to inflation, same size of the house can be priced higher.
Data Drift    refer to when the input distribution (X) changes.</description>
    </item>
    
    <item>
      <title>Measuring Success In A Product</title>
      <link>https://muhammadagf.github.io/posts/notes/measuring-success-in-a-product/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/measuring-success-in-a-product/</guid>
      <description>Measuring the success of a product or its features is a crucial aspect of working on a product. It involves understanding the goals, setting clear success metrics, evaluating actions and metrics, and correlating them with user and business success.
Starting Strategy    To measure the success of a product or its features, we can utilize the following framework:
 Clarify and Ask Questions: Begin by seeking clarity on the goals and objectives of the product.</description>
    </item>
    
    <item>
      <title>Adam (Adaptive Moment Estimation)</title>
      <link>https://muhammadagf.github.io/posts/notes/adam/</link>
      <pubDate>Wed, 10 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/adam/</guid>
      <description>$$w_t = w_{t-1} - \eta \ \dfrac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$ **with:** $$\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1}$$ $$\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}$$ this is for bias correction for the fact that first and second moment estimates start at zero.
given: $$m_t = \beta_1m_{t-1} + (1 - \beta_1)\ \delta w_t$$ $$v_t = \beta_2v_{t-1} + (1 - \beta_2)\ \delta w_t^2$$
 $m_t$ is momentum. $v_t$ takes the idea from AdaGrad / RMSProp  parameter:</description>
    </item>
    
    <item>
      <title>Recurrent Neural Net</title>
      <link>https://muhammadagf.github.io/posts/notes/recurrent-neural-net/</link>
      <pubDate>Sun, 07 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/recurrent-neural-net/</guid>
      <description>Vanilla RNN:    $$h_t = F(h_{t-1}, x_t)$$ $$h_t = activation(W_h\ h_{t-1} + W_x\ x_t)$$ $$y_t = W_y \ h_t$$
image source: https://youtu.be/6niqTuYFZLQ
Problem:     hard to remember long sentences Vanishing and Exploding Gradient problem  LSTM:    image from https://colah.github.io/posts/2015-08-Understanding-LSTMs/
Forget gate:    since i&amp;rsquo;s using sigmoid function the output would be 0-1 (decide how much we should forget the long term memory, if 0 forget)</description>
    </item>
    
    <item>
      <title>Convolutional Neural Net</title>
      <link>https://muhammadagf.github.io/posts/notes/convolutional-neural-net/</link>
      <pubDate>Sat, 06 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/convolutional-neural-net/</guid>
      <description>CNNs use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers. Why CNN?     curse of dimensionality  for a single 32x32 pixel image, it will have 3 channel of 1024 pixels. Fully Connected Neural Networks (FCNNs) treat every pixel as an independent input and do not consider the spatial relationship between them. FCNNs require a large number of parameters, making them computationally expensive and prone to overfitting.</description>
    </item>
    
    <item>
      <title>Missing Values</title>
      <link>https://muhammadagf.github.io/posts/notes/missing-values/</link>
      <pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/missing-values/</guid>
      <description>Type of Missing Data      Structural deficiencies in the data For example: when null means &amp;ldquo;no&amp;rdquo; or 0.
  Random occurrences
  Missing Completely at Random (MCAR) The likelihood of a missing results is equal for all data points. the missing value are independent of the data.
  Missing at Random (MAR) The likelihood of a missing results is not equal for all data points.</description>
    </item>
    
    <item>
      <title>Tree-Based Imputation</title>
      <link>https://muhammadagf.github.io/posts/notes/tree-based-imputation/</link>
      <pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/tree-based-imputation/</guid>
      <description>It involves building a decision tree model using the non-missing data and then using the tree model to predict the missing values. Here are some pros and cons of tree-based imputation:
Pros:
 It can handle non-linear relationships between the variables and non-monotonic missing data patterns. It can impute both continuous and categorical variables. It can be less prone to bias compared to other imputation methods.  Cons:
 It can be computationally intensive, especially when dealing with large datasets.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://muhammadagf.github.io/portofolio/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/portofolio/</guid>
      <description>Projects:  Subtitler | Voice, Text MMD Dance Rythm Game | Vision MLOPS with Kubernetes 0 to 1 Address Elements Extraction | NLP, Competition RealTime Vehicle Counting and Tracking | Vision   Talks:  How Data Science is Boosting Ruangguru to Solve Complex EdTech Problems Recommender System in Education Modeling Student’s Understanding Level of a Concept with Knowledge Tracing      Projects:    Subtitler | Voice, Text    Uses the speech_recognition library to record audio from the selected microphone input.</description>
    </item>
    
    <item>
      <title>Central Limit Theorem</title>
      <link>https://muhammadagf.github.io/posts/notes/central-limit-theorem/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/central-limit-theorem/</guid>
      <description>In probability theory, the central limit theorem (CLT) establishes that, in many situations, for identically distributed independent samples, the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed.
sampling distribution of the mean    given a population with mean $\mu$ and standard deviation $\sigma$, the sampling distribution of the mean approaches: $$\mathcal{N}(\mu, \frac{\sigma}{\sqrt{n}}) $$ as n, the sample size, increases.</description>
    </item>
    
    <item>
      <title>Gradient Boosting</title>
      <link>https://muhammadagf.github.io/posts/notes/gradient-boosting/</link>
      <pubDate>Mon, 27 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/gradient-boosting/</guid>
      <description>Algorithm     Initialize model with a constant value $F_0$ $$F_0(x) = arg\ min\ \gamma \sum_{i}^n L(y_i, \gamma)$$ *initiate the base model (for regression usually $F_0$ is $mean(y)$ For $m$ to $M$:  compute pseudo-residuals: $$r_{im} = \Biggl[ \frac{\partial L(y_i, F(x_i)) }{\partial F(x_i)} \Biggr]_{F(x) = F_{m-1}(x)}$$ for $i = 1, . . ., n.$ Fit a weak learner to pseudo-residuals, i.e. train it using $r_{im}$ as the target.</description>
    </item>
    
    <item>
      <title>Decision Tree</title>
      <link>https://muhammadagf.github.io/posts/notes/decision-tree/</link>
      <pubDate>Sat, 25 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/decision-tree/</guid>
      <description>Greedy, Top Down, Recursive Partitioning
CART Algorithm    CART constructs binary trees using the feature and threshold that yield the largest information gain (usually it based on entropy) at each node.
 get list of split candidates for each split candidates, calculate the purity of the split select the best split, and check if stopping criterion is met repeat step 1  Classification    if a target is a classification with k number of classes, for node m, let prediction: $$p_{mk} = \frac{1}{n_m} \sum_{y\in Node_m} I(y=k)$$ usually, we use Gini Impurity to meassure the purity of each node or split candidates: $$IG(Node) = 1-\sum_{i=0}^k p(i)^2$$ or for binary classification = (1 - probability of yes squared + probability of no squared)</description>
    </item>
    
    <item>
      <title>Muhammad Abdullah Assagaf</title>
      <link>https://muhammadagf.github.io/about/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/about/</guid>
      <description>Hello
I’m currently a Senior Software Engineer at Expedock, where I lead data platform teams focused on data model migrations, resolving discrepancies, and optimizing data onboarding for clients. My work involves designing Business Intelligence solutions using AWS, PostgreSQL, Python, and React, all while building efficient and scalable systems that support complex operations.
Before joining Expedock, I served as a Senior Expert Machine Learning Engineer at Noice. There, I developed and implemented a data orchestration platform using Kubernetes, Docker, and Jenkins.</description>
    </item>
    
    <item>
      <title>Standard Error</title>
      <link>https://muhammadagf.github.io/posts/notes/standard-error/</link>
      <pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/standard-error/</guid>
      <description>usually standard error is: standard deviations of the sampling mean
in essence it gives us how much variations in our &amp;ldquo;means&amp;rdquo; if we took a bunch of independent measurement samples.
formula: $$SE(\hat{x} ) = \frac{\sigma}{\sqrt{n}} $$ $\hat{x}$ is sample mean $\sigma$ is population std $n$ is number of sample
derivation:    $$SE(\hat{x}) = Std(samplemean) = \sqrt{VAR[\hat{X}]}$$ $$\sqrt{VAR[\frac{T}{n}]} = \sqrt{\frac{1}{n^2}VAR[T]} = \sqrt{\frac{1}{n}\sigma^2} = \frac{\sigma}{\sqrt{n}}$$ $T$ is sum of the mean from random variable $\hat{X}$ notice that $T$ consist of sum of iid ( independent and identically distributed) RV because we get T by sampling from the same distribution over and over again (bootstraping).</description>
    </item>
    
    <item>
      <title>Binomial Variable</title>
      <link>https://muhammadagf.github.io/posts/notes/binomial-variable/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/binomial-variable/</guid>
      <description>special class of random variable
X = number of success given N trial with p probability of each success on each trial where each trial are independent with each other.
properties:     made of independent trials each trials can be classified as success or failure fixed # of trials probability of success on each trial is constant  Binomial distribution    special case of probability mass function from a binomial variable pmf if the probability of success is p, the probability of having exactly k success given n trial is given by:</description>
    </item>
    
    <item>
      <title>Permutations And Combinations</title>
      <link>https://muhammadagf.github.io/posts/notes/permutations-and-combinations/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/permutations-and-combinations/</guid>
      <description>permutation    how many way we can arange thing if we care about the order? $$_kP_n = \frac{n!}{(n-k)!}$$ example: how many way 5 person can be seated to seat numbered from 1-3 = $_3P_5$ answer = 5 way for the first seat, 4 way for the 2nd seat, 3 way for the 3rd seat $$\frac{5!}{(5-3)!} = \frac{5!}{2!} = 5 * 4 * 3 = 60$$
combination    how many way we can get item/thing if we care about the order?</description>
    </item>
    
    <item>
      <title>Probability Mass Function (Pmf)</title>
      <link>https://muhammadagf.github.io/posts/notes/probability-mass-function/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/probability-mass-function/</guid>
      <description>PMF for a disscrete random variable X: $p(X)$
Mean (Expectation)    *the sum of the weighted possible values of X (sum of $p(X=x)x$ ) .notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Tip
$$\mu = E[X] = \sum_{x} x p(X=x)$$
 Variance    measure the spread of a PMF (Average Squared Distance from the mean) Tip</description>
    </item>
    
    <item>
      <title>Random Variable</title>
      <link>https://muhammadagf.github.io/posts/notes/random-variable/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/random-variable/</guid>
      <description>mapping outcomes -&amp;gt; numbers from a random process
Discrete    Suppose X is a discrete event, we denote probability of $X=x$ by $p(X=x)$, or just $p(x)$ for short. Here $p(x)$ is called a probability mass function or pmf.
Continuous    Suppose X is some uncertain continuous quantity. The probability that X lies in any interval $a \leq X \leq b$ can be computed as follows:</description>
    </item>
    
    <item>
      <title>Z-Score</title>
      <link>https://muhammadagf.github.io/posts/notes/z-score/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/z-score/</guid>
      <description>how many $\sigma$ (standard deviation) away from the mean from a normal distribution ? $$ z = \frac{x-\mu}{\sigma}$$
usage    comparing 2 distribution for example Juwan took LSAT and MCAT with score: 172 on the LSAT and 37 on the MCAT. summary stats of LSAT and MCAT:
   Exam mean $\mu$ std $\sigma$     LSAT 151 10   MCAT 25.1 6.4    which exam did he do relatively better on?</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://muhammadagf.github.io/posts/notes/linear-regression/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/linear-regression/</guid>
      <description>to answer:
 Is there a relationship between feature advertising budget and sales? How strong is the relationship between advertising budget and sales? Which media are associated with sales? How large is the association between each medium and slaes? How accurately we can predict future sales? Is there synergy among the advertising media?  Single Linear Regression    $$ Y \approx \beta_0 + \beta_1 X$$
RSS (Residual Sum Squared)    let $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$ be the prediction of $y$ based on the $i$th value of $X$.</description>
    </item>
    
    <item>
      <title>Greedy Feature Selection</title>
      <link>https://muhammadagf.github.io/posts/notes/greedy-feature-selection/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/greedy-feature-selection/</guid>
      <description>Simple Filters    The most basic approach to feature selection is to screen the predictors to see if any have a relationship with the outcome prior to including them in a model.
Some popular techniques:
Categorical Feature     when the target is categorical the relationship between feature and outcome forms a contingency tables.  3 or more level for the feature: can use chi-squared test or exact methods 2 level for the feature: can use odds-ratio   when the target is numeric  2 level for the feature: basic t-test can be calculated 3 level for the feature: traditional ANOVA F-statistic can be calculated    Numeric Feature     when the target is categorical similar as when feature is categorical and the target is numeric (but the role is reversed) correlation-adjusted t-scores are a good alternative to simple ANOVA statistics.</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>https://muhammadagf.github.io/posts/notes/entropy/</link>
      <pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/entropy/</guid>
      <description>the entropy of a random variable $X$ with distribution $p$, denoted by $H(X)$ or sometimes $H(p)$, is a measure of it&amp;rsquo;s uncertainty. In particular, for a discrete variable with K states, it is defined by .notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Tip
$$ H(X) \triangleq - \sum_{i=1}^K p(X=k) log(p(X=k)) $$
 in essence entropy is calculating the expected surprise we get from a RV intuitively the value of a surprise is inverse probability of an event, or mathematicaly: $$ surprise(p(X=k)) = log(\frac{1}{p(X=k)})$$</description>
    </item>
    
    <item>
      <title>Feature Selection</title>
      <link>https://muhammadagf.github.io/posts/notes/feature-selection/</link>
      <pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/feature-selection/</guid>
      <description>Goals     Some models (such as SVM and Neural Networks) are sensitive to irrelevant features/predictors. superfluous features can sink predictive performance in some situations. Some models (such as logistic regression) are vulnerable to correlated features. removing features can reduce cost and it make scientific sense to include the minimum possible set that provides acceptable results.  Classes of Feature Selection Method    in general feature selection method can be divided by 3</description>
    </item>
    
    <item>
      <title>Bias Variance Tradeoff</title>
      <link>https://muhammadagf.github.io/posts/notes/bias-variance-tradeoff/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/bias-variance-tradeoff/</guid>
      <description>.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative} Note
MSE = bias$^2$ + variance
 image source: https://www.codingninjas.com/codestudio/library/bias-variance-tradeoff
Bias (Underfitting)    $$Bias[Y_{pred}|Y_{true}] = E[Y_{pred}] - Y_{true}$$ the **bias** error is an error from incorrect assumption in the learning algorithm. high bias can cause an algorithm to miss the relevant relations between features and target output (**underfitting**) (systematic off-the mark)</description>
    </item>
    
    <item>
      <title>K-Means Clustering</title>
      <link>https://muhammadagf.github.io/posts/notes/k-means-clustering/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/k-means-clustering/</guid>
      <description>How it works:
 K centroid are created randomly (based predefined number of K) by selecting K random data as centroid K-means algorithm will allocate each data point in the data to the nearest centroid based on some distance metrics (usually euclidean distance) K-means take the mean of each data in each cluster and use that mean as the new centroid (not necessarily assigned to a single datapoint) K-means then repeat step 2 and 3 until stoping criterion is met (eg.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://muhammadagf.github.io/posts/notes/logistic-regression/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/logistic-regression/</guid>
      <description>statistical model that models the probability of an event taking place by having the log-odds of an event be a linear combination of one or more independent variables. since logistic regression fit a model in the form $p(y|x)$ directly, it&amp;rsquo;s called discriminative approach.
$$ p(y|x,w) = Ber(y|sigm(w^Tx))$$ where sigm is a sigmoid function: $$sigm(x) = \frac{1}{1 + e^{-x}}$$
model fitting    MLE    the negative log-likelihood for logistic regression is given by $$NLL(w) = -\sum_{i=1}^N y_i log(\mu_i) + (1-y_i)log(1-\mu_i)$$ this is also called **cross entropy** error function.</description>
    </item>
    
    <item>
      <title>Regularization</title>
      <link>https://muhammadagf.github.io/posts/notes/regularization/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://muhammadagf.github.io/posts/notes/regularization/</guid>
      <description>add penalty to the loss function and reduce the value of weight. (introduce bias) $$ Regularized loss = loss + penalty $$ example:
L1 Regularization    or called Lasso add “absolute value of magnitude” coefficient as a penalty term to the loss function (minimize sum of of coefficients/weights)
$$ penalty = \lambda ||w|| = \lambda \sum_{j=1}^M |w|$$
pros:     robust to outliers works best when your model contains a lot of useless features (built-in feature selection) preferred when having a high number of features as it&amp;rsquo;s provide sparse solution  cons:     if there is a high group of highly correlated variables, L1 tend to select 1 variable from the group and ignore the others  L2 Regularization    or called Ridge add “squared magnitude” coefficient as a penalty term to the loss function (minimize sum of square of coefficients/weights)</description>
    </item>
    
  </channel>
</rss>
